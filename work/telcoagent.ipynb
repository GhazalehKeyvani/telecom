{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f50c344-ae1e-4b14-ab54-3ce36fe97991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m389.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic)\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m295.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m452.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m912.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-extensions, annotated-types, typing-inspection, pydantic-core, pydantic\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "Successfully installed annotated-types-0.7.0 pydantic-2.11.7 pydantic-core-2.33.2 typing-extensions-4.14.0 typing-inspection-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pydantic requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbe20fd-4a6f-4624-88a0-dca6ecdbb23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.0.0\n",
      "alembic @ file:///home/conda/feedstock_root/build_artifacts/alembic_1694690212886/work\n",
      "altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1696364485230/work\n",
      "annotated-types==0.7.0\n",
      "anyio @ file:///home/conda/feedstock_root/build_artifacts/anyio_1693488585952/work\n",
      "argon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1692818318753/work\n",
      "argon2-cffi-bindings @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi-bindings_1695386553988/work\n",
      "arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1696128962909/work\n",
      "asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1694046349000/work\n",
      "astunparse==1.6.3\n",
      "async-generator==1.10\n",
      "async-lru @ file:///home/conda/feedstock_root/build_artifacts/async-lru_1690563019058/work\n",
      "attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1683424013410/work\n",
      "Babel @ file:///home/conda/feedstock_root/build_artifacts/babel_1696976437817/work\n",
      "backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work\n",
      "backports.functools-lru-cache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1687772187254/work\n",
      "beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1680888073205/work\n",
      "bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1696630167146/work\n",
      "blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1696764509355/work\n",
      "bokeh @ file:///home/conda/feedstock_root/build_artifacts/bokeh_1697011463973/work\n",
      "boltons @ file:///home/conda/feedstock_root/build_artifacts/boltons_1677499911949/work\n",
      "Bottleneck @ file:///home/conda/feedstock_root/build_artifacts/bottleneck_1696018006395/work\n",
      "Brotli @ file:///home/conda/feedstock_root/build_artifacts/brotli-split_1695989787169/work\n",
      "cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work\n",
      "cachetools==5.3.1\n",
      "certifi==2023.7.22\n",
      "certipy==0.1.3\n",
      "cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1696001724357/work\n",
      "charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1696431134479/work\n",
      "click @ file:///home/conda/feedstock_root/build_artifacts/click_1692311806742/work\n",
      "cloudpickle @ file:///home/conda/feedstock_root/build_artifacts/cloudpickle_1697464713350/work\n",
      "colorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1666700638685/work\n",
      "comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1691044910542/work\n",
      "conda @ file:///home/conda/feedstock_root/build_artifacts/conda_1696225914875/work\n",
      "conda-package-handling @ file:///home/conda/feedstock_root/build_artifacts/conda-package-handling_1691048088238/work\n",
      "conda_package_streaming @ file:///home/conda/feedstock_root/build_artifacts/conda-package-streaming_1691009212940/work\n",
      "contourpy @ file:///home/conda/feedstock_root/build_artifacts/contourpy_1695554207611/work\n",
      "cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography-split_1695163784773/work\n",
      "cycler @ file:///home/conda/feedstock_root/build_artifacts/cycler_1696677705766/work\n",
      "Cython @ file:///home/conda/feedstock_root/build_artifacts/cython_1697605111404/work\n",
      "cytoolz @ file:///home/conda/feedstock_root/build_artifacts/cytoolz_1695545149561/work\n",
      "dask @ file:///home/conda/feedstock_root/build_artifacts/dask-core_1697245951516/work\n",
      "debugpy @ file:///home/conda/feedstock_root/build_artifacts/debugpy_1695534290440/work\n",
      "decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work\n",
      "defusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work\n",
      "dill @ file:///home/conda/feedstock_root/build_artifacts/dill_1690101045195/work\n",
      "distributed @ file:///home/conda/feedstock_root/build_artifacts/distributed_1697249144856/work\n",
      "entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work\n",
      "et-xmlfile @ file:///home/conda/feedstock_root/build_artifacts/et_xmlfile_1674664118162/work\n",
      "exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1692026125334/work\n",
      "executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1667317341051/work\n",
      "fastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/python-fastjsonschema_1696171779618/work/dist\n",
      "flatbuffers==23.5.26\n",
      "fonttools @ file:///home/conda/feedstock_root/build_artifacts/fonttools_1696601281595/work\n",
      "fqdn @ file:///home/conda/feedstock_root/build_artifacts/fqdn_1638810296540/work/dist\n",
      "fsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1695417469930/work\n",
      "gast==0.5.4\n",
      "gitdb @ file:///home/conda/feedstock_root/build_artifacts/gitdb_1669279893622/work\n",
      "GitPython @ file:///home/conda/feedstock_root/build_artifacts/gitpython_1697650329377/work\n",
      "gmpy2 @ file:///home/conda/feedstock_root/build_artifacts/gmpy2_1666808665953/work\n",
      "google-auth==2.23.3\n",
      "google-auth-oauthlib==1.0.0\n",
      "google-pasta==0.2.0\n",
      "greenlet @ file:///home/conda/feedstock_root/build_artifacts/greenlet_1696597334170/work\n",
      "grpcio==1.59.0\n",
      "h5py @ file:///home/conda/feedstock_root/build_artifacts/h5py_1696885505601/work\n",
      "idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work\n",
      "imagecodecs @ file:///home/conda/feedstock_root/build_artifacts/imagecodecs_1696191619262/work\n",
      "imageio @ file:///home/conda/feedstock_root/build_artifacts/imageio_1696854106455/work\n",
      "importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1688754491823/work\n",
      "importlib-resources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1695414790617/work\n",
      "ipykernel @ file:///home/conda/feedstock_root/build_artifacts/ipykernel_1693880262622/work\n",
      "ipympl @ file:///home/conda/feedstock_root/build_artifacts/ipympl_1676535632179/work\n",
      "ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1696264049390/work\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets @ file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1694607144474/work\n",
      "isoduration @ file:///home/conda/feedstock_root/build_artifacts/isoduration_1638811571363/work/dist\n",
      "jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1696326070614/work\n",
      "Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work\n",
      "joblib @ file:///home/conda/feedstock_root/build_artifacts/joblib_1691577114857/work\n",
      "json5 @ file:///home/conda/feedstock_root/build_artifacts/json5_1688248289187/work\n",
      "jsonpatch @ file:///home/conda/feedstock_root/build_artifacts/jsonpatch_1695536281965/work\n",
      "jsonpointer @ file:///home/conda/feedstock_root/build_artifacts/jsonpointer_1695397236330/work\n",
      "jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-meta_1695228989494/work\n",
      "jsonschema-specifications @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-specifications_1689701150890/work\n",
      "jupyter-events @ file:///home/conda/feedstock_root/build_artifacts/jupyter_events_1697461661078/work\n",
      "jupyter-lsp @ file:///home/conda/feedstock_root/build_artifacts/jupyter-lsp-meta_1685453365113/work/jupyter-lsp\n",
      "jupyter-server-mathjax @ file:///home/conda/feedstock_root/build_artifacts/jupyter-server-mathjax_1672324512570/work\n",
      "jupyter-telemetry @ file:///home/conda/feedstock_root/build_artifacts/jupyter_telemetry_1605173804246/work\n",
      "jupyter_client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1697036793436/work\n",
      "jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1696974210157/work\n",
      "jupyter_server @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_1697462025510/work\n",
      "jupyter_server_terminals @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_terminals_1673491454549/work\n",
      "jupyterhub @ file:///home/conda/feedstock_root/build_artifacts/jupyterhub-feedstock_1691671359799/work\n",
      "jupyterlab @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_1697059705708/work\n",
      "jupyterlab-git @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab-git_1671178507108/work\n",
      "jupyterlab-pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1649936611996/work\n",
      "jupyterlab-widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1694598704522/work\n",
      "jupyterlab_server @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_server-split_1694531968623/work\n",
      "keras==2.14.0\n",
      "kiwisolver @ file:///home/conda/feedstock_root/build_artifacts/kiwisolver_1695379920604/work\n",
      "lazy_loader @ file:///home/conda/feedstock_root/build_artifacts/lazy_loader_1692295373316/work\n",
      "libclang==16.0.6\n",
      "libmambapy @ file:///home/conda/feedstock_root/build_artifacts/mamba-split_1697482692941/work/libmambapy\n",
      "llvmlite==0.40.1\n",
      "locket @ file:///home/conda/feedstock_root/build_artifacts/locket_1650660393415/work\n",
      "lz4 @ file:///home/conda/feedstock_root/build_artifacts/lz4_1695448708452/work\n",
      "Mako @ file:///home/conda/feedstock_root/build_artifacts/mako_1668568582731/work\n",
      "mamba @ file:///home/conda/feedstock_root/build_artifacts/mamba-split_1697482692941/work/mamba\n",
      "Markdown==3.5\n",
      "MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1695367421802/work\n",
      "matplotlib @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-suite_1697011600307/work\n",
      "matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work\n",
      "mistune @ file:///home/conda/feedstock_root/build_artifacts/mistune_1692116650819/work\n",
      "ml-dtypes==0.2.0\n",
      "mpmath @ file:///home/conda/feedstock_root/build_artifacts/mpmath_1678228039184/work\n",
      "msgpack @ file:///home/conda/feedstock_root/build_artifacts/msgpack-python_1695464112248/work\n",
      "munkres==1.1.4\n",
      "nbclassic @ file:///home/conda/feedstock_root/build_artifacts/nbclassic_1683202081046/work\n",
      "nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1684790896106/work\n",
      "nbconvert @ file:///home/conda/feedstock_root/build_artifacts/nbconvert-meta_1696472732413/work\n",
      "nbdime @ file:///home/conda/feedstock_root/build_artifacts/nbdime_1682876851950/work\n",
      "nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1690814868471/work\n",
      "nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1697083700168/work\n",
      "networkx @ file:///home/conda/feedstock_root/build_artifacts/networkx_1697702021490/work\n",
      "notebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1697550696415/work\n",
      "notebook_shim @ file:///home/conda/feedstock_root/build_artifacts/notebook-shim_1682360583588/work\n",
      "numba @ file:///home/conda/feedstock_root/build_artifacts/numba_1687804756633/work\n",
      "numexpr @ file:///home/conda/feedstock_root/build_artifacts/numexpr_1697637016001/work\n",
      "numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1687808322243/work\n",
      "oauthlib @ file:///home/conda/feedstock_root/build_artifacts/oauthlib_1666056362788/work\n",
      "openpyxl @ file:///home/conda/feedstock_root/build_artifacts/openpyxl_1695464696880/work\n",
      "opt-einsum==3.3.0\n",
      "overrides @ file:///home/conda/feedstock_root/build_artifacts/overrides_1691338815398/work\n",
      "packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1696202382185/work\n",
      "pamela @ file:///home/conda/feedstock_root/build_artifacts/pamela_1691565434937/work\n",
      "pandas @ file:///home/conda/feedstock_root/build_artifacts/pandas_1696030104526/work\n",
      "pandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work\n",
      "parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work\n",
      "partd @ file:///home/conda/feedstock_root/build_artifacts/partd_1695667515973/work\n",
      "patsy @ file:///home/conda/feedstock_root/build_artifacts/patsy_1665356157073/work\n",
      "pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1667297516076/work\n",
      "pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work\n",
      "Pillow @ file:///home/conda/feedstock_root/build_artifacts/pillow_1697423637300/work\n",
      "pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1694617248815/work\n",
      "platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1696272223550/work\n",
      "pluggy @ file:///home/conda/feedstock_root/build_artifacts/pluggy_1693086607691/work\n",
      "prometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1689032443210/work\n",
      "prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1688565951714/work\n",
      "protobuf==4.24.3\n",
      "psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1695367159033/work\n",
      "ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\n",
      "pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work\n",
      "py-cpuinfo @ file:///home/conda/feedstock_root/build_artifacts/py-cpuinfo_1666774466606/work\n",
      "pyarrow==13.0.0\n",
      "pyasn1==0.5.0\n",
      "pyasn1-modules==0.3.0\n",
      "pycosat @ file:///home/conda/feedstock_root/build_artifacts/pycosat_1696355758146/work\n",
      "pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work\n",
      "pycurl==7.45.1\n",
      "pydantic==2.11.7\n",
      "pydantic_core==2.33.2\n",
      "Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1691408637400/work\n",
      "PyJWT @ file:///home/conda/feedstock_root/build_artifacts/pyjwt_1689721553971/work\n",
      "pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1685514481738/work\n",
      "pyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1690737849915/work\n",
      "PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1661604839144/work\n",
      "python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work\n",
      "python-json-logger @ file:///home/conda/feedstock_root/build_artifacts/python-json-logger_1677079630776/work\n",
      "pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1693930252784/work\n",
      "PyWavelets @ file:///home/conda/feedstock_root/build_artifacts/pywavelets_1695567566807/work\n",
      "PyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1695373611984/work\n",
      "pyzmq @ file:///home/conda/feedstock_root/build_artifacts/pyzmq_1695384312770/work\n",
      "referencing @ file:///home/conda/feedstock_root/build_artifacts/referencing_1691337268233/work\n",
      "requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1684774241324/work\n",
      "requests-oauthlib==1.3.1\n",
      "rfc3339-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3339-validator_1638811747357/work\n",
      "rfc3986-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3986-validator_1598024191506/work\n",
      "rpds-py @ file:///home/conda/feedstock_root/build_artifacts/rpds-py_1697072234495/work\n",
      "rsa==4.9\n",
      "ruamel.yaml @ file:///home/conda/feedstock_root/build_artifacts/ruamel.yaml_1697743515120/work\n",
      "ruamel.yaml.clib @ file:///home/conda/feedstock_root/build_artifacts/ruamel.yaml.clib_1695996834452/work\n",
      "scikit-image @ file:///home/conda/feedstock_root/build_artifacts/scikit-image_1697028611470/work/dist/scikit_image-0.22.0-cp311-cp311-linux_x86_64.whl#sha256=53d8b95f752df47007e9e71dd1c9805b9334e1e4791cf48e3762abb922636f04\n",
      "scikit-learn @ file:///home/conda/feedstock_root/build_artifacts/scikit-learn_1696574834166/work\n",
      "scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy-split_1696467621138/work/dist/scipy-1.11.3-cp311-cp311-linux_x86_64.whl#sha256=b226522707b433941a1f15e1b3bb2751e63c9f9c13ca4f89a5e523eb321433d1\n",
      "seaborn @ file:///home/conda/feedstock_root/build_artifacts/seaborn-split_1696262444380/work\n",
      "Send2Trash @ file:///home/conda/feedstock_root/build_artifacts/send2trash_1682601222253/work\n",
      "six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work\n",
      "smmap @ file:///home/conda/feedstock_root/build_artifacts/smmap_1611376390914/work\n",
      "sniffio @ file:///home/conda/feedstock_root/build_artifacts/sniffio_1662051266223/work\n",
      "sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work\n",
      "soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1693929250441/work\n",
      "SQLAlchemy @ file:///home/conda/feedstock_root/build_artifacts/sqlalchemy_1697201147541/work\n",
      "stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work\n",
      "statsmodels @ file:///home/conda/feedstock_root/build_artifacts/statsmodels_1696548154836/work\n",
      "sympy @ file:///home/conda/feedstock_root/build_artifacts/sympy_1684180540116/work\n",
      "tables @ file:///home/conda/feedstock_root/build_artifacts/pytables_1696670791697/work\n",
      "tblib @ file:///home/conda/feedstock_root/build_artifacts/tblib_1694702375735/work\n",
      "tensorboard==2.14.1\n",
      "tensorboard-data-server==0.7.1\n",
      "tensorflow==2.14.0\n",
      "tensorflow-estimator==2.14.0\n",
      "tensorflow-io-gcs-filesystem==0.34.0\n",
      "termcolor==2.3.0\n",
      "terminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1670253674810/work\n",
      "threadpoolctl @ file:///home/conda/feedstock_root/build_artifacts/threadpoolctl_1689261241048/work\n",
      "tifffile @ file:///home/conda/feedstock_root/build_artifacts/tifffile_1695815014864/work\n",
      "tinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1666100256010/work\n",
      "tomli @ file:///home/conda/feedstock_root/build_artifacts/tomli_1644342247877/work\n",
      "toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work\n",
      "tornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1695373450800/work\n",
      "tqdm @ file:///home/conda/feedstock_root/build_artifacts/tqdm_1691671248568/work\n",
      "traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1696377679271/work\n",
      "truststore @ file:///home/conda/feedstock_root/build_artifacts/truststore_1694154605758/work\n",
      "types-python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/types-python-dateutil_1689882883784/work\n",
      "typing-inspection==0.4.1\n",
      "typing-utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1622899189314/work\n",
      "typing_extensions==4.14.0\n",
      "tzdata @ file:///home/conda/feedstock_root/build_artifacts/python-tzdata_1680081134351/work\n",
      "uri-template @ file:///home/conda/feedstock_root/build_artifacts/uri-template_1688655812972/work/dist\n",
      "urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1697720414277/work\n",
      "wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1696255154857/work\n",
      "webcolors @ file:///home/conda/feedstock_root/build_artifacts/webcolors_1679900785843/work\n",
      "webencodings @ file:///home/conda/feedstock_root/build_artifacts/webencodings_1694681268211/work\n",
      "websocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1696770128353/work\n",
      "Werkzeug==3.0.0\n",
      "widgetsnbextension @ file:///home/conda/feedstock_root/build_artifacts/widgetsnbextension_1694598693908/work\n",
      "wrapt==1.14.1\n",
      "xlrd @ file:///home/conda/feedstock_root/build_artifacts/xlrd_1610224409810/work\n",
      "xyzservices @ file:///home/conda/feedstock_root/build_artifacts/xyzservices_1696506440459/work\n",
      "zict @ file:///home/conda/feedstock_root/build_artifacts/zict_1681770155528/work\n",
      "zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1695255097490/work\n",
      "zstandard==0.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c1dd2e0-6c04-4d21-9b06-29890ee26d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic-ai\n",
      "  Downloading pydantic_ai-0.3.4-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic-ai-slim==0.3.4 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading pydantic_ai_slim-0.3.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting eval-type-backport>=0.2.0 (from pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting griffe>=1.3.2 (from pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting httpx>=0.27 (from pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting opentelemetry-api>=1.28.0 (from pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pydantic-graph==0.3.4 (from pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading pydantic_graph-0.3.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pydantic>=2.10 in /opt/conda/lib/python3.11/site-packages (from pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (2.11.7)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (0.4.1)\n",
      "Collecting fasta2a==0.3.4 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading fasta2a-0.3.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting anthropic>=0.52.0 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading anthropic-0.55.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting boto3>=1.37.24 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading boto3-1.38.44-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting argcomplete>=3.5.0 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3 in /opt/conda/lib/python3.11/site-packages (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (3.0.39)\n",
      "Collecting rich>=13 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cohere>=5.13.11 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading cohere-5.15.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pydantic-evals==0.3.4 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading pydantic_evals-0.3.4-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting google-genai>=1.15.0 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading google_genai-1.22.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting groq>=0.19.0 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading groq-0.29.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting mcp>=1.9.4 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading mcp-1.9.4-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting mistralai>=1.2.5 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading mistralai-1.8.2-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting openai>=1.76.0 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading openai-1.91.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting google-auth>=2.36.0 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting requests>=2.32.2 (from pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting starlette>0.29.0 (from fasta2a==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading starlette-0.47.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: anyio>=0 in /opt/conda/lib/python3.11/site-packages (from pydantic-evals==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (4.0.0)\n",
      "Collecting logfire-api>=1.2.0 (from pydantic-evals==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading logfire_api-3.21.1-py3-none-any.whl.metadata (972 bytes)\n",
      "Collecting pyyaml>=6.0.2 (from pydantic-evals==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic>=0.52.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from anthropic>=0.52.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /opt/conda/lib/python3.11/site-packages (from anthropic>=0.52.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (4.14.0)\n",
      "Collecting botocore<1.39.0,>=1.38.44 (from boto3>=1.37.24->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading botocore-1.38.44-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.37.24->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3>=1.37.24->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /opt/conda/lib/python3.11/site-packages (from cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (2.33.2)\n",
      "Collecting tokenizers<1,>=0.15 (from cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth>=2.36.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth>=2.36.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth>=2.36.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (4.9)\n",
      "Collecting anyio>=0 (from pydantic-evals==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting tenacity<9.0.0,>=8.2.3 (from google-genai>=1.15.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai>=1.15.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: colorama>=0.4 in /opt/conda/lib/python3.11/site-packages (from griffe>=1.3.2->pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (0.4.6)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx>=0.27->pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx>=0.27->pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx>=0.27->pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (3.4)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pydantic-settings>=2.5.2 (from mcp>=1.9.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from mcp>=1.9.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp>=1.9.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading sse_starlette-2.3.6-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting uvicorn>=0.23.1 (from mcp>=1.9.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from mistralai>=1.2.5->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (2.8.2)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai>=1.76.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (4.66.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-api>=1.28.0->pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (6.8.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit>=3->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (0.2.8)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.10->pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (0.7.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (2.0.7)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=13->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=13->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (2.16.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim==0.3.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (3.17.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.36.0->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (0.5.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.5.2->mcp>=1.9.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->mistralai>=1.2.5->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (1.16.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading huggingface_hub-0.33.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn>=0.23.1->mcp>=1.9.4->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (8.1.7)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai) (23.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere>=5.13.11->pydantic-ai-slim[a2a,anthropic,bedrock,cli,cohere,evals,google,groq,mcp,mistral,openai,vertexai]==0.3.4->pydantic-ai)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Downloading pydantic_ai-0.3.4-py3-none-any.whl (10 kB)\n",
      "Downloading pydantic_ai_slim-0.3.4-py3-none-any.whl (210 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.5/210.5 kB\u001b[0m \u001b[31m204.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fasta2a-0.3.4-py3-none-any.whl (15 kB)\n",
      "Downloading pydantic_evals-0.3.4-py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m426.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_graph-0.3.4-py3-none-any.whl (27 kB)\n",
      "Downloading anthropic-0.55.0-py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.3/289.3 kB\u001b[0m \u001b[31m194.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m547.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.38.44-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m194.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cohere-5.15.0-py3-none-any.whl (259 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m107.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.1/216.1 kB\u001b[0m \u001b[31m305.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_genai-1.22.0-py3-none-any.whl (222 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.8/222.8 kB\u001b[0m \u001b[31m276.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m206.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading groq-0.29.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m297.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m496.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m405.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mcp-1.9.4-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m185.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mistralai-1.8.2-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.1/374.1 kB\u001b[0m \u001b[31m205.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.91.0-py3-none-any.whl (735 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.8/735.8 kB\u001b[0m \u001b[31m275.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m735.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m372.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m347.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.38.44-py3-none-any.whl (13.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.2/352.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading logfire_api-3.21.1-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m985.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m903.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m819.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sse_starlette-2.3.6-py3-none-any.whl (10 kB)\n",
      "Downloading starlette-0.47.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
      "Downloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.4/515.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/6d/2f/6cad7b5fe86b7652579346cb7f85156c11761df26435651cbba89376cd2c/hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)'))': /packages/6d/2f/6cad7b5fe86b7652579346cb7f85156c11761df26435651cbba89376cd2c/hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0mDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: websockets, types-requests, tenacity, requests, pyyaml, python-multipart, python-dotenv, mdurl, logfire-api, jmespath, jiter, httpx-sse, hf-xet, h11, griffe, filelock, fastavro, eval-type-backport, distro, argcomplete, anyio, uvicorn, starlette, sse-starlette, opentelemetry-api, markdown-it-py, huggingface-hub, httpcore, google-auth, botocore, tokenizers, s3transfer, rich, pydantic-settings, httpx, fasta2a, pydantic-graph, openai, mistralai, mcp, groq, google-genai, cohere, boto3, anthropic, pydantic-ai-slim, pydantic-evals, pydantic-ai\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.23.3\n",
      "    Uninstalling google-auth-2.23.3:\n",
      "      Successfully uninstalled google-auth-2.23.3\n",
      "Successfully installed anthropic-0.55.0 anyio-4.9.0 argcomplete-3.6.2 boto3-1.38.44 botocore-1.38.44 cohere-5.15.0 distro-1.9.0 eval-type-backport-0.2.2 fasta2a-0.3.4 fastavro-1.11.1 filelock-3.18.0 google-auth-2.40.3 google-genai-1.22.0 griffe-1.7.3 groq-0.29.0 h11-0.16.0 hf-xet-1.1.5 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.0 huggingface-hub-0.33.1 jiter-0.10.0 jmespath-1.0.1 logfire-api-3.21.1 markdown-it-py-3.0.0 mcp-1.9.4 mdurl-0.1.2 mistralai-1.8.2 openai-1.91.0 opentelemetry-api-1.34.1 pydantic-ai-0.3.4 pydantic-ai-slim-0.3.4 pydantic-evals-0.3.4 pydantic-graph-0.3.4 pydantic-settings-2.10.1 python-dotenv-1.1.1 python-multipart-0.0.20 pyyaml-6.0.2 requests-2.32.4 rich-14.0.0 s3transfer-0.13.0 sse-starlette-2.3.6 starlette-0.47.1 tenacity-8.5.0 tokenizers-0.21.2 types-requests-2.32.4.20250611 uvicorn-0.34.3 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pydantic-ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef5a5802-4181-4b29-a4d1-9a7efa32e950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter region to analyze (e.g., EMEA, APAC, NA):  EMEA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "📊 SALES REPORT FOR: EMEA\n",
      "==================================================\n",
      "\n",
      "🔍 Summary:\n",
      "این دسته بازاریابی ها از ۱۰۰۰ فروش در منطقه EMEA و ۸۰۰ فروش دارای محصول C هستند.\n",
      "\n",
      "💡 Key Insights:\n",
      "  1. • هیچ بارهایی از محصولات A و C وجود نداشتند در EMEA کشور.\n",
      "  2. • توزیع محصولات C بسیار ارزشمند است.\n",
      "  3. • با توجه به تاثیر محصول C، شرکت میتواند محصول های دیگری را چسبنده و با ابزارهای مختلف بازاریابی داشته باشد.\n",
      "  4. • با این حال، بررسی بیشتری در مورد این محصولات باید انجام شود تا بهتر قرار دهد.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Define your schemas\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class QueryInput(BaseModel):\n",
    "    \"\"\"What to fetch—could be SQL, ElasticDSL, a search term, etc.\"\"\"\n",
    "    query: str = Field(..., description=\"SQL query or search keyword\")\n",
    "\n",
    "class QueryOutput(BaseModel):\n",
    "    \"\"\"Raw rows or records as a list of dicts.\"\"\"\n",
    "    rows: list[dict] = Field(..., description=\"List of data records\")\n",
    "\n",
    "class ReportInput(BaseModel):\n",
    "    \"\"\"Takes the fetched rows and emits a formatted report.\"\"\"\n",
    "    rows: list[dict]\n",
    "\n",
    "class ReportOutput(BaseModel):\n",
    "    \"\"\"A plain-text summary for end-users.\"\"\"\n",
    "    report: str = Field(..., description=\"Summary report\")\n",
    "\n",
    "class AnalysisInput(BaseModel):\n",
    "    \"\"\"Takes the same raw rows and emits structured insights.\"\"\"\n",
    "    rows: list[dict]\n",
    "\n",
    "class AnalysisOutput(BaseModel):\n",
    "    \"\"\"List of key insights, each as a string.\"\"\"\n",
    "    insights: list[str] = Field(..., description=\"Actionable insights\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Custom LLM function\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def custom_llm(prompt: str) -> str:\n",
    "    \"\"\"Call local Ollama API with streaming response handling\"\"\"\n",
    "    url = \"http://host.docker.internal:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": \"qwen2:1.5b\",\n",
    "        \"prompt\": 'به فارسی جواب بده ' + prompt\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=data, stream=True, timeout=30)\n",
    "        response.raise_for_status()  # Raise HTTP errors\n",
    "        \n",
    "        final_output = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:  # Filter out keep-alive new lines\n",
    "                try:\n",
    "                    json_data = json.loads(line.decode(\"utf-8\"))\n",
    "                    final_output += json_data.get(\"response\", \"\")\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # Skip invalid JSON lines\n",
    "        return final_output\n",
    "    except requests.RequestException as e:\n",
    "        return f\"LLM Error: {str(e)}\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Implement each agent\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class QueryAgent:\n",
    "    \"\"\"Fetches data from any external source (DB, REST, etc.).\"\"\"\n",
    "    def run(self, inputs: QueryInput) -> QueryOutput:\n",
    "        # Fake database - replace with real data source\n",
    "        fake_db = [\n",
    "            {\"region\": \"EMEA\", \"sales\": 1_000, \"product\": \"A\"},\n",
    "            {\"region\": \"APAC\", \"sales\": 1_500, \"product\": \"B\"},\n",
    "            {\"region\": \"NA\",   \"sales\": 2_000, \"product\": \"A\"},\n",
    "            {\"region\": \"EMEA\", \"sales\": 800,  \"product\": \"C\"},\n",
    "        ]\n",
    "        # Filter by query\n",
    "        rows = [r for r in fake_db if inputs.query.lower() in r[\"region\"].lower()]\n",
    "        return QueryOutput(rows=rows)\n",
    "\n",
    "class ReportAgent:\n",
    "    \"\"\"Turns raw rows into a prose report via LLM.\"\"\"\n",
    "    def run(self, inputs: ReportInput) -> ReportOutput:\n",
    "        prompt = f\"\"\"\n",
    "        You are a sales analyst assistant. Write a concise one-paragraph summary\n",
    "        of these sales records in Persian: {inputs.rows}\n",
    "        \"\"\"\n",
    "        summary = custom_llm(prompt).strip()\n",
    "        return ReportOutput(report=summary)\n",
    "\n",
    "class AnalysisAgent:\n",
    "    \"\"\"Generates a bulleted list of deeper insights.\"\"\"\n",
    "    def run(self, inputs: AnalysisInput) -> AnalysisOutput:\n",
    "        prompt = f\"\"\"\n",
    "        Given these sales records {inputs.rows},\n",
    "        provide 3–5 bullet points of insights or recommended next steps in Persian.\n",
    "        Format each insight on a new line starting with '• '.\n",
    "        \"\"\"\n",
    "        raw_analysis = custom_llm(prompt).strip()\n",
    "        \n",
    "        # Parse bullet points\n",
    "        insights = [\n",
    "            line.strip() \n",
    "            for line in raw_analysis.split('\\n') \n",
    "            if line.strip() and line.strip().startswith(('•', '-', '*', '‣'))\n",
    "        ]\n",
    "        \n",
    "        # Fallback if bullet parsing fails\n",
    "        if not insights:\n",
    "            insights = [raw_analysis]\n",
    "            \n",
    "        return AnalysisOutput(insights=insights)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Main execution flow\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def main():\n",
    "    # Initialize agents\n",
    "    qa = QueryAgent()\n",
    "    ra = ReportAgent()\n",
    "    aa = AnalysisAgent()\n",
    "\n",
    "    # Get user query\n",
    "    user_query = input(\"Enter region to analyze (e.g., EMEA, APAC, NA): \").strip()\n",
    "    \n",
    "    # Execute pipeline\n",
    "    try:\n",
    "        qout = qa.run(QueryInput(query=user_query))\n",
    "        \n",
    "        if not qout.rows:\n",
    "            print(\"\\n⚠️ No data found for this region\")\n",
    "            return\n",
    "            \n",
    "        rout = ra.run(ReportInput(rows=qout.rows))\n",
    "        aout = aa.run(AnalysisInput(rows=qout.rows))\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"📊 SALES REPORT FOR: {user_query.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\n🔍 Summary:\")\n",
    "        print(rout.report)\n",
    "        \n",
    "        print(\"\\n💡 Key Insights:\")\n",
    "        for i, insight in enumerate(aout.insights, 1):\n",
    "            print(f\"  {i}. {insight}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61db92ed-4a67-4890-8923-8f1cbe23f206",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'telecom_churn.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the telecom churn dataset\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtelecom_churn.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Display dataset information\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'telecom_churn.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "\n",
    "# Load the telecom churn dataset\n",
    "df = pd.read_csv('telecom_churn.csv')\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"First 3 rows:\\n{df.head(3)}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Custom LLM function\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def custom_llm(prompt: str) -> str:\n",
    "    \"\"\"Call local Ollama API with streaming response handling\"\"\"\n",
    "    url = \"http://host.docker.internal:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": \"qwen2:1.5b\",\n",
    "        \"prompt\": 'به فارسی جواب بده ' + prompt\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=data, stream=True, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        final_output = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                try:\n",
    "                    json_data = json.loads(line.decode(\"utf-8\"))\n",
    "                    final_output += json_data.get(\"response\", \"\")\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        return final_output\n",
    "    except requests.RequestException as e:\n",
    "        return f\"LLM Error: {str(e)}\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Define schemas\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class QueryInput(BaseModel):\n",
    "    query: str = Field(..., description=\"Query to filter telecom data\")\n",
    "\n",
    "class QueryOutput(BaseModel):\n",
    "    rows: List[Dict] = Field(..., description=\"Filtered telecom records\")\n",
    "\n",
    "class ReportInput(BaseModel):\n",
    "    rows: List[Dict]\n",
    "\n",
    "class ReportOutput(BaseModel):\n",
    "    report: str = Field(..., description=\"Summary report\")\n",
    "\n",
    "class AnalysisInput(BaseModel):\n",
    "    rows: List[Dict]\n",
    "\n",
    "class AnalysisOutput(BaseModel):\n",
    "    insights: List[str] = Field(..., description=\"Actionable insights\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Implement agents\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class QueryAgent:\n",
    "    \"\"\"Fetches telecom churn data based on query\"\"\"\n",
    "    def run(self, inputs: QueryInput) -> QueryOutput:\n",
    "        try:\n",
    "            # Convert query to lowercase for case-insensitive matching\n",
    "            query = inputs.query.lower()\n",
    "            \n",
    "            # Filter based on different criteria\n",
    "            if query == \"all\":\n",
    "                filtered = df\n",
    "            elif query == \"churned\":\n",
    "                filtered = df[df['Churn'] == True]\n",
    "            elif query == \"active\":\n",
    "                filtered = df[df['Churn'] == False]\n",
    "            elif len(query) == 2 and query.isalpha():  # State code\n",
    "                filtered = df[df['State'].str.lower() == query]\n",
    "            elif query.isdigit() and len(query) == 3:  # Area code\n",
    "                filtered = df[df['Area code'] == int(query)]\n",
    "            else:\n",
    "                # Try to match in multiple columns\n",
    "                filtered = df[\n",
    "                    df['State'].str.lower().str.contains(query) |\n",
    "                    df['International plan'].str.lower().str.contains(query) |\n",
    "                    df['Voice mail plan'].str.lower().str.contains(query)\n",
    "                ]\n",
    "            \n",
    "            # Convert to list of dictionaries and limit to 100 records\n",
    "            records = filtered.head(100).to_dict('records')\n",
    "            return QueryOutput(rows=records)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Query error: {str(e)}\")\n",
    "            return QueryOutput(rows=[])\n",
    "\n",
    "class ReportAgent:\n",
    "    \"\"\"Generates telecom summary report\"\"\"\n",
    "    def run(self, inputs: ReportInput) -> ReportOutput:\n",
    "        prompt = f\"\"\"\n",
    "        You are a telecom analyst. Write a concise summary in Persian about \n",
    "        these customer records. Focus on churn rates, key features, and patterns.\n",
    "        Data: {inputs.rows[:3]}  # Show first 3 records as sample\n",
    "        Total records: {len(inputs.rows)}\n",
    "        \"\"\"\n",
    "        summary = custom_llm(prompt).strip()\n",
    "        return ReportOutput(report=summary)\n",
    "\n",
    "class AnalysisAgent:\n",
    "    \"\"\"Generates insights from telecom data\"\"\"\n",
    "    def run(self, inputs: AnalysisInput) -> AnalysisOutput:\n",
    "        prompt = f\"\"\"\n",
    "        Analyze these telecom customer records and provide 3-5 key insights \n",
    "        in Persian with bullet points. Focus on churn patterns and recommendations.\n",
    "        Start each insight with '• '. \n",
    "        Data sample: {inputs.rows[:2]}\n",
    "        Total records: {len(inputs.rows)}\n",
    "        \"\"\"\n",
    "        raw_analysis = custom_llm(prompt).strip()\n",
    "        \n",
    "        # Parse bullet points\n",
    "        insights = [\n",
    "            line.strip() \n",
    "            for line in raw_analysis.split('\\n') \n",
    "            if line.strip().startswith(('•', '-', '*', '‣'))\n",
    "        ]\n",
    "        \n",
    "        # Fallback if bullet parsing fails\n",
    "        if not insights:\n",
    "            insights = [raw_analysis]\n",
    "            \n",
    "        return AnalysisOutput(insights=insights)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Main execution flow\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def main():\n",
    "    # Initialize agents\n",
    "    qa = QueryAgent()\n",
    "    ra = ReportAgent()\n",
    "    aa = AnalysisAgent()\n",
    "    \n",
    "    # Display query options\n",
    "    print(\"\\n\" + \"═\"*50)\n",
    "    print(\"📞 Telecom Churn Analysis System\")\n",
    "    print(\"═\"*50)\n",
    "    print(\"Query options:\")\n",
    "    print(\"- 'all': All customers\")\n",
    "    print(\"- 'churned': Churned customers\")\n",
    "    print(\"- 'active': Active customers\")\n",
    "    print(\"- State code (e.g., 'ca', 'ny')\")\n",
    "    print(\"- Area code (e.g., '408', '510')\")\n",
    "    print(\"- Plan type (e.g., 'yes', 'no')\")\n",
    "    print(\"═\"*50)\n",
    "    \n",
    "    # Get user query\n",
    "    user_query = input(\"Enter your query: \").strip()\n",
    "    \n",
    "    # Execute pipeline\n",
    "    try:\n",
    "        print(\"\\n⚡ Processing query...\")\n",
    "        qout = qa.run(QueryInput(query=user_query))\n",
    "        \n",
    "        if not qout.rows:\n",
    "            print(\"\\n⚠️ No data found for this query\")\n",
    "            return\n",
    "            \n",
    "        print(f\"✅ Found {len(qout.rows)} records\")\n",
    "        \n",
    "        print(\"\\n📝 Generating report...\")\n",
    "        rout = ra.run(ReportInput(rows=qout.rows))\n",
    "        \n",
    "        print(\"\\n🔍 Analyzing insights...\")\n",
    "        aout = aa.run(AnalysisInput(rows=qout.rows))\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"📊 TELECOM ANALYSIS REPORT: {user_query.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\n📄 Summary Report:\")\n",
    "        print(rout.report)\n",
    "        \n",
    "        print(\"\\n💡 Key Insights:\")\n",
    "        for i, insight in enumerate(aout.insights, 1):\n",
    "            print(f\"  {i}. {insight}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b3be7d-1f3f-4851-8307-a87d9ecd50ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
      "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
      "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
      "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
      "3  7795-CFOCW    Male              0      No         No      45           No   \n",
      "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
      "\n",
      "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
      "0  No phone service             DSL             No  ...               No   \n",
      "1                No             DSL            Yes  ...              Yes   \n",
      "2                No             DSL            Yes  ...               No   \n",
      "3  No phone service             DSL            Yes  ...              Yes   \n",
      "4                No     Fiber optic             No  ...               No   \n",
      "\n",
      "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
      "0          No          No              No  Month-to-month              Yes   \n",
      "1          No          No              No        One year               No   \n",
      "2          No          No              No  Month-to-month              Yes   \n",
      "3         Yes          No              No        One year               No   \n",
      "4          No          No              No  Month-to-month              Yes   \n",
      "\n",
      "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
      "0           Electronic check          29.85         29.85    No  \n",
      "1               Mailed check          56.95        1889.5    No  \n",
      "2               Mailed check          53.85        108.15   Yes  \n",
      "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
      "4           Electronic check          70.70        151.65   Yes  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a CSV file\n",
    "df = pd.read_csv('/work/telecom_churn.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab599c-eaa8-4a7e-ab17-0f27ed78d273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded successfully!\n",
      "📊 Shape: (7043, 21)\n",
      "📝 Columns: ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
      "\n",
      "==================================================\n",
      "📊 سیستم تحلیل داده‌های مشترکین مخابراتی\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 سوال خود را وارد کنید (یا 'exit' برای خروج):  مشتریان ریزشی چه ویژگی‌هایی دارند؟\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 سوال بهبودیافته: \"متاسفانه، به عنوان یک هوش مصنوعی من نمی‌توانم این سوال را درست بازنویسی کنم. لطفاً مشتریان ریزشی را بپرسید تا بتوانم به شما کمک کنم.\"\n",
      "\n",
      "🔄 در حال تحلیل داده‌ها...\n",
      "\n",
      "==================================================\n",
      "💡 نتایج تحلیل:\n",
      "با توجه به سوال کاربر، شما می‌توانید از داده‌ها برای محاسبه نرخ ریزش مشتری استفاده کنید. در اینجا، نمایشگر چندگانه (Pie Chart) به عنوان نمودار مناسب برای نمایش نتایج را پیشنهاد می‌کنم.\n",
      "\n",
      "برخی از نتایج:\n",
      "- نرخ ریزش مشتری: 20%\n",
      "- نرخ ریزش مشتری با استفاده از ARPU: 30%\n",
      "- نمودار نرخ ریزش مشتری:\n",
      "\n",
      "![نمودار نرخ ریزش مشتری](https://i.imgur.com/7Wn5bY1.png)\n",
      "\n",
      "این نمودار با استفاده از ARPU، نموداری برای نمایش نرخ ریزش مشتری و تاثیر آن بر ARPU را ارائه می‌دهد.\n",
      "\n",
      "📊 پیشنهاد مصورسازی:\n",
      "نمودار دایره‌ای\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 سوال خود را وارد کنید (یا 'exit' برای خروج):  مشتریان ریزشی چه ویژگی‌هایی دارند؟ با مثال\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 سوال بهبودیافته: \"مشتریان ریزشی چه ویژگی‌هایی دارند؟ با مثال?\"\n",
      "\n",
      "🔄 در حال تحلیل داده‌ها...\n",
      "\n",
      "==================================================\n",
      "💡 نتایج تحلیل:\n",
      "با توجه به سوال کاربر، شما می‌توانید اطلاعاتی درباره مشتریان ریزشی را بررسی کنید. این مشتریان در یک دوره زمانی دسترسی به خدمات مخابراتی دارند و همچنین نسبت به مشتریان دیگر دارای تفاوت‌هایی هستند.\n",
      "\n",
      "برای انجام این کار، شما باید از داده‌های موجود استفاده کنید. این داده‌ها شامل ۵ ردیف اولیه می‌باشند:\n",
      "\n",
      "1. customerID\n",
      "2. gender\n",
      "3. SeniorCitizen\n",
      "4. Partner\n",
      "5. Dependents\n",
      "6. tenure\n",
      "7. PhoneService\n",
      "8. MultipleLines\n",
      "9. InternetService\n",
      "10. OnlineSecurity\n",
      "11. OnlineBackup\n",
      "12. DeviceProtection\n",
      "13. TechSupport\n",
      "14. StreamingTV\n",
      "15. StreamingMovies\n",
      "16. Contract\n",
      "17. PaperlessBilling\n",
      "18. PaymentMethod\n",
      "19. MonthlyCharges\n",
      "20. TotalCharges\n",
      "21. Churn\n",
      "\n",
      "برای تحلیل این داده‌ها، شما می‌توانید از چندین روش مختلف استفاده کنید. برخی از این روش ها شامل:\n",
      "\n",
      "1. تجزیه و تحلیل نمودار: این روش به منظور پردازش داده‌ها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "2. تحلیل مدل‌های دستگاه‌های شناختی: این روش برای پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "3. تحلیل بازاریابی: این روش برای پردازش داده‌های موجود و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "4. تحلیل نمودارها: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "5. تحلیل سازگاری بازاریابی: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "6. تحلیل نمودارها برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "7. تحلیل سازگاری بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "8. تحلیل بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "9. تحلیل بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "10. تحلیل بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "11. تحلیل بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "12. تحلیل بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "13. تحلیل بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "14. تحلیل بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "15. تحلیل بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "16. تحلیل بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "17. تحلیل بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "18. تحلیل بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "19. تحلیل بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "20. تحلیل بازاریابی برای توضیحات: این روش به منظور پردازش نمودارها و تجزیه آن‌ها در مورد مشتریان ریزشی استفاده می‌شود.\n",
      "\n",
      "📊 پیشنهاد مصورسازی:\n",
      "نمودار دایره‌ای\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Load telecom churn dataset\n",
    "try:\n",
    "    df = pd.read_csv('/work/telecom_churn.csv')\n",
    "    print(\"✅ Dataset loaded successfully!\")\n",
    "    print(f\"📊 Shape: {df.shape}\")\n",
    "    print(f\"📝 Columns: {list(df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: telecom_churn.csv file not found\")\n",
    "    exit()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Enhanced LLM function with caching and improved error handling\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "LLM_CACHE = {}\n",
    "\n",
    "def custom_llm(prompt: str, use_cache: bool = True) -> str:\n",
    "    \"\"\"Call local Ollama API with enhanced features\"\"\"\n",
    "    # Check cache first\n",
    "    if use_cache and prompt in LLM_CACHE:\n",
    "        return LLM_CACHE[prompt]\n",
    "    \n",
    "    url = \"http://host.docker.internal:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": \"qwen2:1.5b\",\n",
    "        \"prompt\": 'به فارسی جواب بده ' + prompt,\n",
    "        \"options\": {\"temperature\": 0.3, \"num_ctx\": 4096}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=data, stream=True, timeout=90)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        final_output = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                try:\n",
    "                    json_data = json.loads(line.decode(\"utf-8\"))\n",
    "                    if \"response\" in json_data:\n",
    "                        final_output += json_data[\"response\"]\n",
    "                    if json_data.get(\"done\", False):\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        \n",
    "        # Cache the result\n",
    "        LLM_CACHE[prompt] = final_output\n",
    "        return final_output\n",
    "    except requests.RequestException as e:\n",
    "        return f\"خطا در ارتباط با مدل زبانی: {str(e)}\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Analysis Agent with intelligent prompt generation\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class AnalysisInput(BaseModel):\n",
    "    query: str = Field(..., description=\"Natural language query about the data\")\n",
    "    sample_size: int = Field(5, description=\"Number of sample rows to include\")\n",
    "\n",
    "class AnalysisOutput(BaseModel):\n",
    "    insights: str = Field(..., description=\"Analysis results in Persian\")\n",
    "    visualization_suggestion: str = Field(..., description=\"Suggested visualization type\")\n",
    "\n",
    "class TelecomAnalysisAgent:\n",
    "    \"\"\"Intelligent agent for telecom churn data analysis\"\"\"\n",
    "    \n",
    "    def generate_analysis_prompt(self, query: str, sample: List[dict]) -> str:\n",
    "        \"\"\"Create optimized prompt for data analysis\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        شما یک تحلیل‌گر داده‌های مشترکین مخابراتی هستید. کاربر سوال زیر را پرسیده است:\n",
    "        «{query}»\n",
    "        \n",
    "        داده‌های نمونه (۵ ردیف اول):\n",
    "        {json.dumps(sample, ensure_ascii=False, indent=2)}\n",
    "        \n",
    "        ساختار کامل داده‌ها:\n",
    "        {list(df.columns)}\n",
    "        \n",
    "        دستورالعمل:\n",
    "        ۱. تحلیل را دقیقاً بر اساس سوال کاربر انجام دهید\n",
    "        ۲. از اصطلاحات تخصصی مخابرات (مثل نرخ ریزش، ARPU، وفاداری مشتری) استفاده کنید\n",
    "        ۳. نتایج را به فارسی ساده و قابل فهم ارائه دهید\n",
    "        ۴. در انتها نوع نمودار مناسب برای نمایش نتایج را پیشنهاد دهید\n",
    "        \n",
    "        تحلیل و نتیجه‌گیری:\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def generate_visualization_prompt(self, insight: str) -> str:\n",
    "        \"\"\"Suggest appropriate visualization based on insight\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        بر اساس این تحلیل: «{insight}»\n",
    "        بهترین نوع نمودار برای نمایش این نتیجه کدام است؟ فقط نام نمودار را بگویید.\n",
    "        \n",
    "        گزینه‌ها:\n",
    "        - نمودار میله‌ای\n",
    "        - نمودار دایره‌ای\n",
    "        - نمودار خطی\n",
    "        - هیستوگرام\n",
    "        - نمودار پراکندگی\n",
    "        - باکس پلات\n",
    "        - هیچکدام\n",
    "        \n",
    "        پاسخ:\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def run(self, inputs: AnalysisInput) -> AnalysisOutput:\n",
    "        # Get data sample\n",
    "        sample = df.head(inputs.sample_size).to_dict('records')\n",
    "        \n",
    "        # Generate analysis prompt\n",
    "        analysis_prompt = self.generate_analysis_prompt(inputs.query, sample)\n",
    "        insight = custom_llm(analysis_prompt).strip()\n",
    "        \n",
    "        # Generate visualization suggestion\n",
    "        viz_prompt = self.generate_visualization_prompt(insight)\n",
    "        visualization = custom_llm(viz_prompt).strip()\n",
    "        \n",
    "        return AnalysisOutput(\n",
    "            insights=insight,\n",
    "            visualization_suggestion=visualization\n",
    "        )\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Query Understanding Agent\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class QueryUnderstandingAgent:\n",
    "    \"\"\"Improves user queries for better analysis\"\"\"\n",
    "    \n",
    "    def enhance_query(self, query: str) -> str:\n",
    "        \"\"\"Refine natural language queries\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        کاربر این سوال را درباره داده‌های مشترکین تلفن همراه پرسیده است:\n",
    "        «{query}»\n",
    "        \n",
    "        این سوال را به صورت حرفه‌ای و دقیق‌تر بازنویسی کن تا برای تحلیل داده‌ها مناسب‌تر باشد. \n",
    "        از اصطلاحات تخصصی حوزه مخابرات استفاده کن.\n",
    "        \n",
    "        سوال بهبودیافته:\n",
    "        \"\"\"\n",
    "        return custom_llm(prompt).strip()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Main Analysis System\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def analyze_telecom_data():\n",
    "    # Initialize agents\n",
    "    analysis_agent = TelecomAnalysisAgent()\n",
    "    query_agent = QueryUnderstandingAgent()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📊 سیستم تحلیل داده‌های مشترکین مخابراتی\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    while True:\n",
    "        # Get user query\n",
    "        user_query = input(\"\\n💬 سوال خود را وارد کنید (یا 'exit' برای خروج): \").strip()\n",
    "        \n",
    "        if user_query.lower() in ['exit', 'خروج']:\n",
    "            break\n",
    "            \n",
    "        if not user_query:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Enhance the query\n",
    "            enhanced_query = query_agent.enhance_query(user_query)\n",
    "            print(f\"\\n🔍 سوال بهبودیافته: {enhanced_query}\")\n",
    "            \n",
    "            # Perform analysis\n",
    "            print(\"\\n🔄 در حال تحلیل داده‌ها...\")\n",
    "            result = analysis_agent.run(AnalysisInput(query=enhanced_query))\n",
    "            \n",
    "            # Display results\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"💡 نتایج تحلیل:\")\n",
    "            print(result.insights)\n",
    "            \n",
    "            print(\"\\n📊 پیشنهاد مصورسازی:\")\n",
    "            print(result.visualization_suggestion)\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ خطا: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_telecom_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27080d-fc42-4d01-8ea6-ab7de93ba3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
